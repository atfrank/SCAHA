train_balance <-  rbind(train_false, train_true[sample(1:nrow(train_true), n_false, replace = TRUE), ]))
# BUILD CLASSIFIER
library(randomForest)
library(caret)
rf <- randomForest(formula = reference_flag~., data = train_balance, na.action = na.exclude, do.trace = TRUE, ntree = 100)
train_balance <-  rbind(train_false, train_true[sample(1:nrow(train_true), n_false, replace = TRUE), ]))
train_balance <-  rbind(train_false, train_true[sample(1:nrow(train_true), n_false, replace = TRUE), ])
# user function
norm_errors <- function(errors){
# normalizes the errors based on the mean
errors$total <- rowSums(errors[,!(colnames(errors) %in% c("id", "model", "reference_flag"))], na.rm = TRUE)
cols <- colnames(errors)[!(colnames(errors) %in% c("id", "model", "reference_flag"))]
for (col in cols){
errors[, col] <- errors[, col]/median(errors[, col])
}
return(errors)
}
scores <- function(predictions, N=10){
# function to quanitfy the ability to correctly classifying models as native or non-native
# returns the NSLR and the fraction of the high probability models that are considered native
require(nmR)
return(data.frame(NSLR=nslr(predictions$reference_flag), TOPN=mean(as.numeric(predictions$reference_flag[1:N])-1)))
}
nslr <- function(X){
#' Sum of Logarithmic Ranks Function
#'
#' This function allows you to compute the normalized sum of logarithmic ranks
#' @param X vector of 0 (inactives) and 1 (actives) that was sorted based on some scores (e.g., agreement between measured and predicted shifts)
#' @export
#' @examples
#' random_nslr(sample(c(rep(0,100),rep(1,10))))
ri <- which(X==1)
N <- length(X)
i <-  1:length(ri)
SLRmax <- -sum(log(i/N)) # logo rank
return(-sum(log(ri/N))/SLRmax)
}
# load errors matrices
load("~/GitSoftware/error_based_cs_classifier/data/errors_consensus_r0.RData")
# scramble data just to be sure
errors <- errors[sample(1:nrow(errors)),]
# normalize the error for each RNA (specificed by id) separately
library(plyr)
library(dplyr)
errors <- ddply(.data = errors, .variables = c("id"), .fun = norm_errors)
errors <- as.data.frame(errors)
# make sure the reference_flag is a factor so that when used in randomForest will recognize this as a classification problem
errors$reference_flag <- as.factor(errors$reference_flag)
# SPLIT INTO TRAINING AND TESTING
# choosing training and testing RNAs
train_rnas <- "1KKA 1L1W 1LC6 1LDZ 1NC0 1OW9 1PJY 1R7W 1SCL 1UUU 1XHP 1YSV 1Z2J 1ZC5 2FDT 2JWV 2K66 2KOC 2L3E 2LBJ 2LBL 2LDL 2LDT 2LHP 2LI4 2LK3 2LP9 2LPA 2LQZ 2LU0 2LUB"
test_rnas <- "2LUN 2LV0 2M12 2M21 2M22 2M24 2M4W 2M5U 2M8K 2MEQ 2MFD 2MHI 2MNC 2MXL 2N2O 2N2P 2N4L 2N6S 2N6T 2N6W 2N6X"
train_rnas <- unlist(strsplit(train_rnas, " "))
test_rnas <- unlist(strsplit(test_rnas, " "))
train <- errors[(errors$id %in% train_rnas), !(colnames(errors) %in% c("id", "model"))]
test <- errors[(errors$id %in% test_rnas), !(colnames(errors) %in% c("id", "model"))]
test_info <- errors[(errors$id %in% test_rnas), (colnames(errors) %in% c("id", "model"))]
train[is.na(train)] <- 9999
test[is.na(test)] <- 9999
# set column names for training and testing dataframes
names <- unlist(strsplit("reference_flag C1p C2p C3p C4p C5p C2 C5 C6 C8 H1p H2p H3p H4p H5p H5pp H2 H5 H6 H8 total", " "))
colnames(train) <- names
colnames(test) <- names
# Logic to balance training set
n_true <- sum(train$reference_flag==1)
n_false <- sum(train$reference_flag==0)
n_minor <- abs(n_true-n_false)
train_true <- subset(train, reference_flag==1)
train_false <- subset(train, reference_flag==0)
#train_balance <-  rbind(train_true, rbind(train_false, train_false[sample(1:nrow(train_false), n_minor, replace = TRUE), ]))
train_balance <-  rbind(train_false, train_true[sample(1:nrow(train_true), n_false, replace = TRUE), ]))
# BUILD CLASSIFIER
library(randomForest)
library(caret)
rf <- randomForest(formula = reference_flag~., data = train_balance, na.action = na.exclude, do.trace = TRUE, ntree = 100)
train_balance <-  rbind(train_false, train_true[sample(1:nrow(train_true), n_false, replace = TRUE), ])
# user function
norm_errors <- function(errors){
# normalizes the errors based on the mean
errors$total <- rowSums(errors[,!(colnames(errors) %in% c("id", "model", "reference_flag"))], na.rm = TRUE)
cols <- colnames(errors)[!(colnames(errors) %in% c("id", "model", "reference_flag"))]
for (col in cols){
errors[, col] <- errors[, col]/median(errors[, col])
}
return(errors)
}
scores <- function(predictions, N=10){
# function to quanitfy the ability to correctly classifying models as native or non-native
# returns the NSLR and the fraction of the high probability models that are considered native
require(nmR)
return(data.frame(NSLR=nslr(predictions$reference_flag), TOPN=mean(as.numeric(predictions$reference_flag[1:N])-1)))
}
nslr <- function(X){
#' Sum of Logarithmic Ranks Function
#'
#' This function allows you to compute the normalized sum of logarithmic ranks
#' @param X vector of 0 (inactives) and 1 (actives) that was sorted based on some scores (e.g., agreement between measured and predicted shifts)
#' @export
#' @examples
#' random_nslr(sample(c(rep(0,100),rep(1,10))))
ri <- which(X==1)
N <- length(X)
i <-  1:length(ri)
SLRmax <- -sum(log(i/N)) # logo rank
return(-sum(log(ri/N))/SLRmax)
}
# load errors matrices
load("~/GitSoftware/error_based_cs_classifier/data/errors_consensus_r0.RData")
# scramble data just to be sure
errors <- errors[sample(1:nrow(errors)),]
# normalize the error for each RNA (specificed by id) separately
library(plyr)
library(dplyr)
errors <- ddply(.data = errors, .variables = c("id"), .fun = norm_errors)
errors <- as.data.frame(errors)
# make sure the reference_flag is a factor so that when used in randomForest will recognize this as a classification problem
errors$reference_flag <- as.factor(errors$reference_flag)
# SPLIT INTO TRAINING AND TESTING
# choosing training and testing RNAs
train_rnas <- "1KKA 1L1W 1LC6 1LDZ 1NC0 1OW9 1PJY 1R7W 1SCL 1UUU 1XHP 1YSV 1Z2J 1ZC5 2FDT 2JWV 2K66 2KOC 2L3E 2LBJ 2LBL 2LDL 2LDT 2LHP 2LI4 2LK3 2LP9 2LPA 2LQZ 2LU0 2LUB"
test_rnas <- "2LUN 2LV0 2M12 2M21 2M22 2M24 2M4W 2M5U 2M8K 2MEQ 2MFD 2MHI 2MNC 2MXL 2N2O 2N2P 2N4L 2N6S 2N6T 2N6W 2N6X"
train_rnas <- unlist(strsplit(train_rnas, " "))
test_rnas <- unlist(strsplit(test_rnas, " "))
train <- errors[(errors$id %in% train_rnas), !(colnames(errors) %in% c("id", "model"))]
test <- errors[(errors$id %in% test_rnas), !(colnames(errors) %in% c("id", "model"))]
test_info <- errors[(errors$id %in% test_rnas), (colnames(errors) %in% c("id", "model"))]
train[is.na(train)] <- 9999
test[is.na(test)] <- 9999
# set column names for training and testing dataframes
names <- unlist(strsplit("reference_flag C1p C2p C3p C4p C5p C2 C5 C6 C8 H1p H2p H3p H4p H5p H5pp H2 H5 H6 H8 total", " "))
colnames(train) <- names
colnames(test) <- names
# Logic to balance training set
n_true <- sum(train$reference_flag==1)
n_false <- sum(train$reference_flag==0)
n_minor <- abs(n_true-n_false)
train_true <- subset(train, reference_flag==1)
train_false <- subset(train, reference_flag==0)
#train_balance <-  rbind(train_true, rbind(train_false, train_false[sample(1:nrow(train_false), n_minor, replace = TRUE), ]))
train_balance <-  rbind(train_false, train_true[sample(1:nrow(train_true), n_false, replace = TRUE), ])
# BUILD CLASSIFIER
library(randomForest)
library(caret)
rf <- randomForest(formula = reference_flag~., data = train_balance, na.action = na.exclude, do.trace = TRUE, ntree = 100)
# Make predictions using classifier
test$reference_flag_binary <- predict(rf, test, type = "response")
test$reference_flag_prob <- predict(rf, test, type = "prob")
t <- confusionMatrix(test$reference_flag, predict(rf, test))
test <- cbind(test_info, test)
# get performance after sorting based on total error
test <- test[order(test[,c("total")], decreasing = FALSE), ]
test_nslr_total_error <- ddply(.data = test, .variables = c("id"), .fun = scores, N=1)
# get nmR package ready to use
#install.packages("nmR")
library("nmR")
# get performance after sorting based on predicted probability of being native
test <- test[order(test[,c("reference_flag_prob")][,1]), ]
test_nslr_class <- ddply(.data = test, .variables = c("id"), .fun = scores, N=1)
# compare results obtained when using the total error and classifier
comp <- cbind(test_nslr_total_error, test_nslr_class)
comp$diff <- test_nslr_total_error$NSLR - test_nslr_class$NSLR
print(comp)
# Questions:
# (1) does a error-based classifier enhance our ability to identify native structures
# (2) is the random forest classifier over kill? That is, can we simiply reweight the contributions of the individual error terms to enhance classification accuracy?
# (3) how would you go about reweighting the contributions from error term?
# I would suggest using a genetic algorithm that uses the NSLR as the fitness function. See the R package: GA
# also see skeleton code in scripts/ga.R
# user function
norm_errors <- function(errors){
# normalizes the errors based on the mean
errors$total <- rowSums(errors[,!(colnames(errors) %in% c("id", "model", "reference_flag"))], na.rm = TRUE)
cols <- colnames(errors)[!(colnames(errors) %in% c("id", "model", "reference_flag"))]
for (col in cols){
errors[, col] <- errors[, col]/median(errors[, col])
}
return(errors)
}
scores <- function(predictions, N=10){
# function to quanitfy the ability to correctly classifying models as native or non-native
# returns the NSLR and the fraction of the high probability models that are considered native
require(nmR)
return(data.frame(NSLR=nslr(predictions$reference_flag), TOPN=mean(as.numeric(predictions$reference_flag[1:N])-1)))
}
nslr <- function(X){
#' Sum of Logarithmic Ranks Function
#'
#' This function allows you to compute the normalized sum of logarithmic ranks
#' @param X vector of 0 (inactives) and 1 (actives) that was sorted based on some scores (e.g., agreement between measured and predicted shifts)
#' @export
#' @examples
#' random_nslr(sample(c(rep(0,100),rep(1,10))))
ri <- which(X==1)
N <- length(X)
i <-  1:length(ri)
SLRmax <- -sum(log(i/N)) # logo rank
return(-sum(log(ri/N))/SLRmax)
}
# load errors matrices
load("~/GitSoftware/error_based_cs_classifier/data/errors_consensus_r0.RData")
# scramble data just to be sure
errors <- errors[sample(1:nrow(errors)),]
# normalize the error for each RNA (specificed by id) separately
library(plyr)
library(dplyr)
errors <- ddply(.data = errors, .variables = c("id"), .fun = norm_errors)
errors <- as.data.frame(errors)
# make sure the reference_flag is a factor so that when used in randomForest will recognize this as a classification problem
errors$reference_flag <- as.factor(errors$reference_flag)
# SPLIT INTO TRAINING AND TESTING
# choosing training and testing RNAs
train_rnas <- "1KKA 1L1W 1LC6 1LDZ 1NC0 1OW9 1PJY 1R7W 1SCL 1UUU 1XHP 1YSV 1Z2J 1ZC5 2FDT 2JWV 2K66 2KOC 2L3E 2LBJ 2LBL 2LDL 2LDT 2LHP 2LI4 2LK3 2LP9 2LPA 2LQZ 2LU0 2LUB"
test_rnas <- "2LUN 2LV0 2M12 2M21 2M22 2M24 2M4W 2M5U 2M8K 2MEQ 2MFD 2MHI 2MNC 2MXL 2N2O 2N2P 2N4L 2N6S 2N6T 2N6W 2N6X"
train_rnas <- unlist(strsplit(train_rnas, " "))
test_rnas <- unlist(strsplit(test_rnas, " "))
train <- errors[(errors$id %in% train_rnas), !(colnames(errors) %in% c("id", "model"))]
test <- errors[(errors$id %in% test_rnas), !(colnames(errors) %in% c("id", "model"))]
test_info <- errors[(errors$id %in% test_rnas), (colnames(errors) %in% c("id", "model"))]
train[is.na(train)] <- 0
test[is.na(test)] <- 0
# set column names for training and testing dataframes
names <- unlist(strsplit("reference_flag C1p C2p C3p C4p C5p C2 C5 C6 C8 H1p H2p H3p H4p H5p H5pp H2 H5 H6 H8 total", " "))
colnames(train) <- names
colnames(test) <- names
# Logic to balance training set
n_true <- sum(train$reference_flag==1)
n_false <- sum(train$reference_flag==0)
n_minor <- abs(n_true-n_false)
train_true <- subset(train, reference_flag==1)
train_false <- subset(train, reference_flag==0)
#train_balance <-  rbind(train_true, rbind(train_false, train_false[sample(1:nrow(train_false), n_minor, replace = TRUE), ]))
train_balance <-  rbind(train_false, train_true[sample(1:nrow(train_true), n_false, replace = TRUE), ])
# BUILD CLASSIFIER
library(randomForest)
library(caret)
rf <- randomForest(formula = reference_flag~., data = train_balance, na.action = na.exclude, do.trace = TRUE, ntree = 100)
# Make predictions using classifier
test$reference_flag_binary <- predict(rf, test, type = "response")
test$reference_flag_prob <- predict(rf, test, type = "prob")
t <- confusionMatrix(test$reference_flag, predict(rf, test))
test <- cbind(test_info, test)
# get performance after sorting based on total error
test <- test[order(test[,c("total")], decreasing = FALSE), ]
test_nslr_total_error <- ddply(.data = test, .variables = c("id"), .fun = scores, N=1)
# get nmR package ready to use
#install.packages("nmR")
library("nmR")
# get performance after sorting based on predicted probability of being native
test <- test[order(test[,c("reference_flag_prob")][,1]), ]
test_nslr_class <- ddply(.data = test, .variables = c("id"), .fun = scores, N=1)
# compare results obtained when using the total error and classifier
comp <- cbind(test_nslr_total_error, test_nslr_class)
comp$diff <- test_nslr_total_error$NSLR - test_nslr_class$NSLR
print(comp)
# Questions:
# (1) does a error-based classifier enhance our ability to identify native structures
# (2) is the random forest classifier over kill? That is, can we simiply reweight the contributions of the individual error terms to enhance classification accuracy?
# (3) how would you go about reweighting the contributions from error term?
# I would suggest using a genetic algorithm that uses the NSLR as the fitness function. See the R package: GA
# also see skeleton code in scripts/ga.R
# normalize the error for each RNA (specificed by id) separately
library(plyr)
library(dplyr)
errors <- ddply(.data = errors, .variables = c("id"), .fun = norm_errors)
errors <- as.data.frame(errors)
# make sure the reference_flag is a factor so that when used in randomForest will recognize this as a classification problem
errors$reference_flag <- as.factor(errors$reference_flag)
# SPLIT INTO TRAINING AND TESTING
# choosing training and testing RNAs
train_rnas <- "1KKA 1L1W 1LC6 1LDZ 1NC0 1OW9 1PJY 1R7W 1SCL 1UUU 1XHP 1YSV 1Z2J 1ZC5 2FDT 2JWV 2K66 2KOC 2L3E 2LBJ 2LBL 2LDL 2LDT 2LHP 2LI4 2LK3 2LP9 2LPA 2LQZ 2LU0 2LUB"
test_rnas <- "2LUN 2LV0 2M12 2M21 2M22 2M24 2M4W 2M5U 2M8K 2MEQ 2MFD 2MHI 2MNC 2MXL 2N2O 2N2P 2N4L 2N6S 2N6T 2N6W 2N6X"
train_rnas <- unlist(strsplit(train_rnas, " "))
test_rnas <- unlist(strsplit(test_rnas, " "))
train <- errors[(errors$id %in% train_rnas), !(colnames(errors) %in% c("id", "model"))]
test <- errors[(errors$id %in% test_rnas), !(colnames(errors) %in% c("id", "model"))]
test_info <- errors[(errors$id %in% test_rnas), (colnames(errors) %in% c("id", "model"))]
dim(complete.cases(train))
dim(train[complete.cases(train),])
dim(test[complete.cases(test),])
complete.cases(test)
complete.cases(test)==TRUE
complete.cases(test)==FALSE
sum(complete.cases(test)==FALSE)
dim(test)
# user function
norm_errors <- function(errors){
# normalizes the errors based on the mean
errors$total <- rowSums(errors[,!(colnames(errors) %in% c("id", "model", "reference_flag"))], na.rm = TRUE)
cols <- colnames(errors)[!(colnames(errors) %in% c("id", "model", "reference_flag"))]
for (col in cols){
errors[, col] <- errors[, col]/median(errors[, col])
}
return(errors)
}
scores <- function(predictions, N=10){
# function to quanitfy the ability to correctly classifying models as native or non-native
# returns the NSLR and the fraction of the high probability models that are considered native
require(nmR)
return(data.frame(NSLR=nslr(predictions$reference_flag), TOPN=mean(as.numeric(predictions$reference_flag[1:N])-1)))
}
nslr <- function(X){
#' Sum of Logarithmic Ranks Function
#'
#' This function allows you to compute the normalized sum of logarithmic ranks
#' @param X vector of 0 (inactives) and 1 (actives) that was sorted based on some scores (e.g., agreement between measured and predicted shifts)
#' @export
#' @examples
#' random_nslr(sample(c(rep(0,100),rep(1,10))))
ri <- which(X==1)
N <- length(X)
i <-  1:length(ri)
SLRmax <- -sum(log(i/N)) # logo rank
return(-sum(log(ri/N))/SLRmax)
}
# load errors matrices
load("~/GitSoftware/error_based_cs_classifier/data/errors_consensus_r0.RData")
# scramble data just to be sure
errors <- errors[sample(1:nrow(errors)),]
# normalize the error for each RNA (specificed by id) separately
library(plyr)
library(dplyr)
errors <- ddply(.data = errors, .variables = c("id"), .fun = norm_errors)
errors <- as.data.frame(errors)
# make sure the reference_flag is a factor so that when used in randomForest will recognize this as a classification problem
errors$reference_flag <- as.factor(errors$reference_flag)
errors <- errors[complete.cases(errors),]
# SPLIT INTO TRAINING AND TESTING
# choosing training and testing RNAs
train_rnas <- "1KKA 1L1W 1LC6 1LDZ 1NC0 1OW9 1PJY 1R7W 1SCL 1UUU 1XHP 1YSV 1Z2J 1ZC5 2FDT 2JWV 2K66 2KOC 2L3E 2LBJ 2LBL 2LDL 2LDT 2LHP 2LI4 2LK3 2LP9 2LPA 2LQZ 2LU0 2LUB"
test_rnas <- "2LUN 2LV0 2M12 2M21 2M22 2M24 2M4W 2M5U 2M8K 2MEQ 2MFD 2MHI 2MNC 2MXL 2N2O 2N2P 2N4L 2N6S 2N6T 2N6W 2N6X"
train_rnas <- unlist(strsplit(train_rnas, " "))
test_rnas <- unlist(strsplit(test_rnas, " "))
train <- errors[(errors$id %in% train_rnas), !(colnames(errors) %in% c("id", "model"))]
test <- errors[(errors$id %in% test_rnas), !(colnames(errors) %in% c("id", "model"))]
test_info <- errors[(errors$id %in% test_rnas), (colnames(errors) %in% c("id", "model"))]
train[is.na(train)] <- 0
test[is.na(test)] <- 0
# set column names for training and testing dataframes
names <- unlist(strsplit("reference_flag C1p C2p C3p C4p C5p C2 C5 C6 C8 H1p H2p H3p H4p H5p H5pp H2 H5 H6 H8 total", " "))
colnames(train) <- names
colnames(test) <- names
# Logic to balance training set
n_true <- sum(train$reference_flag==1)
n_false <- sum(train$reference_flag==0)
n_minor <- abs(n_true-n_false)
train_true <- subset(train, reference_flag==1)
train_false <- subset(train, reference_flag==0)
#train_balance <-  rbind(train_true, rbind(train_false, train_false[sample(1:nrow(train_false), n_minor, replace = TRUE), ]))
train_balance <-  rbind(train_false, train_true[sample(1:nrow(train_true), n_false, replace = TRUE), ])
# BUILD CLASSIFIER
library(randomForest)
library(caret)
rf <- randomForest(formula = reference_flag~., data = train_balance, na.action = na.exclude, do.trace = TRUE, ntree = 100)
# Make predictions using classifier
test$reference_flag_binary <- predict(rf, test, type = "response")
test$reference_flag_prob <- predict(rf, test, type = "prob")
t <- confusionMatrix(test$reference_flag, predict(rf, test))
test <- cbind(test_info, test)
# get performance after sorting based on total error
test <- test[order(test[,c("total")], decreasing = FALSE), ]
test_nslr_total_error <- ddply(.data = test, .variables = c("id"), .fun = scores, N=1)
# get nmR package ready to use
#install.packages("nmR")
library("nmR")
# get performance after sorting based on predicted probability of being native
test <- test[order(test[,c("reference_flag_prob")][,1]), ]
test_nslr_class <- ddply(.data = test, .variables = c("id"), .fun = scores, N=1)
# compare results obtained when using the total error and classifier
comp <- cbind(test_nslr_total_error, test_nslr_class)
comp$diff <- test_nslr_total_error$NSLR - test_nslr_class$NSLR
print(comp)
# Questions:
# (1) does a error-based classifier enhance our ability to identify native structures
# (2) is the random forest classifier over kill? That is, can we simiply reweight the contributions of the individual error terms to enhance classification accuracy?
# (3) how would you go about reweighting the contributions from error term?
# I would suggest using a genetic algorithm that uses the NSLR as the fitness function. See the R package: GA
# also see skeleton code in scripts/ga.R
t
test
test_nslr_total_error
head(test[order(test[,c("reference_flag_prob")][,1]), ])
# compare results obtained when using the total error and classifier
comp <- cbind(test_nslr_total_error, test_nslr_class)
comp$diff <- test_nslr_total_error$NSLR - test_nslr_class$NSLR
print(comp)
# user function
norm_errors <- function(errors){
# normalizes the errors based on the mean
errors$total <- rowSums(errors[,!(colnames(errors) %in% c("id", "model", "reference_flag"))], na.rm = TRUE)
cols <- colnames(errors)[!(colnames(errors) %in% c("id", "model", "reference_flag"))]
for (col in cols){
errors[, col] <- errors[, col]/median(errors[, col])
}
return(errors)
}
scores <- function(predictions, N=10){
# function to quanitfy the ability to correctly classifying models as native or non-native
# returns the NSLR and the fraction of the high probability models that are considered native
require(nmR)
return(data.frame(NSLR=nslr(predictions$reference_flag), TOPN=mean(as.numeric(predictions$reference_flag[1:N])-1)))
}
nslr <- function(X){
#' Sum of Logarithmic Ranks Function
#'
#' This function allows you to compute the normalized sum of logarithmic ranks
#' @param X vector of 0 (inactives) and 1 (actives) that was sorted based on some scores (e.g., agreement between measured and predicted shifts)
#' @export
#' @examples
#' random_nslr(sample(c(rep(0,100),rep(1,10))))
ri <- which(X==1)
N <- length(X)
i <-  1:length(ri)
SLRmax <- -sum(log(i/N)) # logo rank
return(-sum(log(ri/N))/SLRmax)
}
# load errors matrices
load("~/GitSoftware/error_based_cs_classifier/data/errors_consensus_r0.RData")
# scramble data just to be sure
errors <- errors[sample(1:nrow(errors)),]
# normalize the error for each RNA (specificed by id) separately
library(plyr)
library(dplyr)
errors <- ddply(.data = errors, .variables = c("id"), .fun = norm_errors)
errors <- as.data.frame(errors)
# make sure the reference_flag is a factor so that when used in randomForest will recognize this as a classification problem
errors$reference_flag <- as.factor(errors$reference_flag)
errors <- errors[complete.cases(errors),]
# SPLIT INTO TRAINING AND TESTING
# choosing training and testing RNAs
train_rnas <- "1KKA 1L1W 1LC6 1LDZ 1NC0 1OW9 1PJY 1R7W 1SCL 1UUU 1XHP 1YSV 1Z2J 1ZC5 2FDT 2JWV 2K66 2KOC 2L3E 2LBJ 2LBL 2LDL 2LDT 2LHP 2LI4 2LK3 2LP9 2LPA 2LQZ 2LU0 2LUB"
test_rnas <- "2LUN 2LV0 2M12 2M21 2M22 2M24 2M4W 2M5U 2M8K 2MEQ 2MFD 2MHI 2MNC 2MXL 2N2O 2N2P 2N4L 2N6S 2N6T 2N6W 2N6X"
train_rnas <- unlist(strsplit(train_rnas, " "))
test_rnas <- unlist(strsplit(test_rnas, " "))
train <- errors[(errors$id %in% train_rnas), !(colnames(errors) %in% c("id", "model"))]
test <- errors[(errors$id %in% test_rnas), !(colnames(errors) %in% c("id", "model"))]
test_info <- errors[(errors$id %in% test_rnas), (colnames(errors) %in% c("id", "model"))]
train[is.na(train)] <- 9999
test[is.na(test)] <- 9999
# set column names for training and testing dataframes
names <- unlist(strsplit("reference_flag C1p C2p C3p C4p C5p C2 C5 C6 C8 H1p H2p H3p H4p H5p H5pp H2 H5 H6 H8 total", " "))
colnames(train) <- names
colnames(test) <- names
# Logic to balance training set
n_true <- sum(train$reference_flag==1)
n_false <- sum(train$reference_flag==0)
n_minor <- abs(n_true-n_false)
train_true <- subset(train, reference_flag==1)
train_false <- subset(train, reference_flag==0)
#train_balance <-  rbind(train_true, rbind(train_false, train_false[sample(1:nrow(train_false), n_minor, replace = TRUE), ]))
train_balance <-  rbind(train_false, train_true[sample(1:nrow(train_true), n_false, replace = TRUE), ])
write.table(train_balance, file="train_balance.txt", col.names = F, row.names = F, quote = F)
write.table(test, file="train_balance.txt", col.names = F, row.names = F, quote = F)
write.table(train_balance, file="train_balance.txt", col.names = F, row.names = F, quote = F)
write.table(test, file="test.txt", col.names = F, row.names = F, quote = F)
getwd()
library(randomForest)
?randomForest
0.8/20
0.6/20
0.6/40
0.6/80
0.6/80
1/3
1/5
source("http://bioconductor.org/biocLite.R") # Sources the biocLite.R installation script.
biocLite("ChemmineR")
library("ChemmineR")
read.SDFset("~/Downloads/E7189DBED3DC190D3DEACE4AA25BFD47ki.sdf")
read.SDFstr("~/Downloads/E7189DBED3DC190D3DEACE4AA25BFD47ki.sdf")
13*4
7*20*4
7*20*4/16
library(nmR)
library("nmR")
# 0 - goto working directory
setwd("~/GitSoftware/SCAHA/manuscript/data_analysis/")
# 1 - source custom function
source("analysis_functions.R")
# 2 - load accuracy data
load("analysis_data.RData")
# 3 - get accuracy
acc <- combine_accuracy_data(data)
count_perfect_assignments(data[["luu"]])
plyr::ddply(.data = data[["luu"]], .variables = c("id"), .fun = count_perfect_assignments)
head(data[["luu"]])
perfect_assignments(data[["luu"]])
source('~/Documents/GitSoftware/SCAHA/manuscript/data_analysis/analysis_functions.R')
source('~/Documents/GitSoftware/SCAHA/manuscript/data_analysis/analysis_functions.R')
perfect_assignments(data[["luu"]])
source('~/Documents/GitSoftware/SCAHA/manuscript/data_analysis/analysis_functions.R')
summarize_perfect_assignment_accuracy(data)
source('~/Documents/GitSoftware/SCAHA/manuscript/data_analysis/analysis_functions.R')
summarize_perfect_assignment_accuracy(data)
perfect_assignments(data[["luu"]])
source('~/Documents/GitSoftware/SCAHA/manuscript/data_analysis/analysis_functions.R')
summarize_perfect_assignment_accuracy(data)
source('~/Documents/GitSoftware/SCAHA/manuscript/data_analysis/analysis_functions.R')
summarize_perfect_assignment_accuracy(data)
source('~/Documents/GitSoftware/SCAHA/manuscript/data_analysis/analysis_functions.R')
summarize_perfect_assignment_accuracy(data)
source('~/Documents/GitSoftware/SCAHA/manuscript/data_analysis/analysis_functions.R')
summarize_perfect_assignment_accuracy(data)
source('~/Documents/GitSoftware/SCAHA/manuscript/data_analysis/analysis_functions.R')
summarize_perfect_assignment_accuracy(data)
source('~/Documents/GitSoftware/SCAHA/manuscript/data_analysis/analysis_functions.R')
fractions <- summarize_perfect_assignment_accuracy(data)
source('~/Documents/GitSoftware/SCAHA/manuscript/data_analysis/analysis_functions.R')
fractions <- summarize_perfect_assignment_accuracy(data)
source('~/Documents/GitSoftware/SCAHA/manuscript/data_analysis/analysis_functions.R')
fractions <- summarize_perfect_assignment_accuracy(data)
source('~/Documents/GitSoftware/SCAHA/manuscript/data_analysis/analysis_functions.R')
fractions <- summarize_perfect_assignment_accuracy(data)
